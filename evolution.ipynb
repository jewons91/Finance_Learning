{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "# UserWarning 무시 (필요 시 제거 가능)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 여부 확인)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Transformer 모듈 정의 (8개의 TransformerEncoderLayer 스택)\n",
    "class TimesNet(nn.Module):\n",
    "    def __init__(self, input_dim, seq_length, embed_dim=128, num_heads=4, num_layers=8, dropout=0.1):\n",
    "        super(TimesNet, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_linear = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_linear(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_linear(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer_norm(x[:, -1, :])\n",
    "\n",
    "        return x\n",
    "\n",
    "# 다중 종목 주식 트레이딩 환경 정의\n",
    "class MultiStockTradingEnv(gym.Env):\n",
    "    def __init__(self, dfs, stock_dim=3, initial_balance=5000000, max_stock=500, seq_length=20):\n",
    "        super(MultiStockTradingEnv, self).__init__()\n",
    "        self.dfs = dfs  \n",
    "        self.stock_dim = stock_dim  \n",
    "        self.initial_balance = initial_balance  \n",
    "        self.max_stock = max_stock \n",
    "        self.transaction_fee = 0.0025  \n",
    "        self.slippage = 0.0005  \n",
    "        self.max_loss = 0.2  \n",
    "        self.seq_length = seq_length  \n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.stock_dim * (7 * self.seq_length + 1) + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.MultiDiscrete([13] * self.stock_dim) \n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.current_step = 0\n",
    "        self.stock_owned = {ticker: {'quantity': 0, 'avg_price': 0} for ticker in self.dfs.keys()}\n",
    "        self.stock_price = {}\n",
    "        self.total_asset = []\n",
    "\n",
    "        self.balance_history = [self.balance]\n",
    "        self.portfolio_value_history = [self.portfolio_value]\n",
    "        self.action_history = []\n",
    "        self.price_history = {ticker: [] for ticker in self.dfs.keys()}\n",
    "        self.trade_history = []  \n",
    "\n",
    "        self.max_steps = min(len(df) for df in self.dfs.values()) - self.seq_length - 1\n",
    "        if self.max_steps <= 0:\n",
    "            raise ValueError(\"데이터프레임의 길이가 시퀀스 길이보다 짧습니다.\")\n",
    "\n",
    "        self.data_indices = {ticker: self.seq_length for ticker in self.dfs.keys()}  \n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = []\n",
    "        for ticker, df in self.dfs.items():\n",
    "            idx = self.data_indices[ticker]\n",
    "            seq = df.loc[idx - self.seq_length:idx - 1, ['Close', 'MA10', 'MA20', 'RSI', 'Volume', 'Upper_Band', 'Lower_Band']].values\n",
    "            obs.extend(seq.flatten())\n",
    "            obs.append(self.stock_owned[ticker]['quantity'])\n",
    "\n",
    "            self.stock_price[ticker] = df.iloc[idx]['Close_unscaled']\n",
    "\n",
    "        obs.append(self.balance)\n",
    "\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "    def step(self, actions):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        invalid_action_penalty = -10  \n",
    "        self.action_history.append(actions)\n",
    "        trade_info = []  \n",
    "\n",
    "        prev_portfolio_value = self.portfolio_value  \n",
    "\n",
    "        action_mapping = {\n",
    "            0: ('sell', 1.0),\n",
    "            1: ('sell', 0.75),\n",
    "            2: ('sell', 0.5),\n",
    "            3: ('sell', 0.25),\n",
    "            4: ('hold', 0.0),\n",
    "            5: ('buy', 0.25),\n",
    "            6: ('buy', 0.5),\n",
    "            7: ('buy', 0.75),\n",
    "            8: ('buy', 1.0)\n",
    "        }\n",
    "\n",
    "        for i, (ticker, df) in enumerate(self.dfs.items()):\n",
    "            action = actions[i]\n",
    "            idx = self.data_indices[ticker]\n",
    "\n",
    "            if idx >= len(df):\n",
    "                done = True\n",
    "                trade_info.append(f\"Ticker {ticker} reached end of data. Ending episode.\")\n",
    "                break\n",
    "\n",
    "            actual_price = df.iloc[idx]['Close_unscaled']\n",
    "\n",
    "            action_type, proportion = action_mapping.get(action, ('hold', 0.0))\n",
    "\n",
    "            if action_type == 'buy':\n",
    "                adjusted_price = actual_price * (1 + self.slippage)\n",
    "            elif action_type == 'sell':\n",
    "                adjusted_price = actual_price * (1 - self.slippage)\n",
    "            else:\n",
    "                adjusted_price = actual_price\n",
    "\n",
    "            fee = adjusted_price * self.transaction_fee\n",
    "\n",
    "            buy_amount = 0\n",
    "            sell_amount = 0\n",
    "\n",
    "            reward = 0  \n",
    "            if action_type == 'sell':\n",
    "                if self.stock_owned[ticker]['quantity'] > 0:\n",
    "                    sell_amount = int(self.stock_owned[ticker]['quantity'] * proportion)\n",
    "                    sell_amount = max(1, sell_amount)  \n",
    "                    sell_amount = min(sell_amount, self.stock_owned[ticker]['quantity'])  \n",
    "                    proceeds = adjusted_price * sell_amount - fee * sell_amount\n",
    "                    self.balance += proceeds\n",
    "                    profit = (adjusted_price - self.stock_owned[ticker]['avg_price']) * sell_amount - fee * sell_amount\n",
    "                    reward = profit  \n",
    "                    self.stock_owned[ticker]['quantity'] -= sell_amount\n",
    "                    if self.stock_owned[ticker]['quantity'] == 0:\n",
    "                        self.stock_owned[ticker]['avg_price'] = 0\n",
    "                    trade_info.append(f\"Sell {sell_amount} of {ticker} at {adjusted_price:.2f}\")\n",
    "                else:\n",
    "                    reward = invalid_action_penalty\n",
    "                    trade_info.append(f\"Cannot Sell {ticker} (No holdings)\")\n",
    "            elif action_type == 'buy':\n",
    "                max_can_buy = min(\n",
    "                    self.max_stock - self.stock_owned[ticker]['quantity'],\n",
    "                    int(self.balance // (adjusted_price + fee))\n",
    "                )\n",
    "                buy_amount = int(max_can_buy * proportion)\n",
    "                buy_amount = max(1, buy_amount)  \n",
    "                buy_amount = min(buy_amount, self.max_stock - self.stock_owned[ticker]['quantity'], \n",
    "                                 int(self.balance // (adjusted_price + fee)))\n",
    "                if buy_amount > 0:\n",
    "                    cost = adjusted_price * buy_amount + fee * buy_amount\n",
    "                    self.balance -= cost\n",
    "                    total_quantity = self.stock_owned[ticker]['quantity'] + buy_amount\n",
    "                    if total_quantity > 0:\n",
    "                        self.stock_owned[ticker]['avg_price'] = (\n",
    "                            (self.stock_owned[ticker]['avg_price'] * self.stock_owned[ticker]['quantity'] + adjusted_price * buy_amount)\n",
    "                            / total_quantity\n",
    "                        )\n",
    "                    self.stock_owned[ticker]['quantity'] = total_quantity\n",
    "                    reward = 0  \n",
    "                    trade_info.append(f\"Buy {buy_amount} of {ticker} at {adjusted_price:.2f}\")\n",
    "                else:\n",
    "                    reward = invalid_action_penalty  \n",
    "                    trade_info.append(f\"Cannot Buy {ticker} (Insufficient balance or max stock)\")\n",
    "            else:\n",
    "                trade_info.append(f\"Hold {ticker}\")\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            self.price_history[ticker].append(actual_price)\n",
    "\n",
    "            self.data_indices[ticker] += 1\n",
    "\n",
    "        self.trade_history.append(trade_info)\n",
    "\n",
    "        self.portfolio_value = self.balance + sum(\n",
    "            self.stock_owned[ticker]['quantity'] * self.stock_price[ticker] for ticker in self.dfs.keys()\n",
    "        )\n",
    "        self.total_asset.append(self.portfolio_value)\n",
    "\n",
    "        if prev_portfolio_value > 0:\n",
    "            portfolio_return = (self.portfolio_value - prev_portfolio_value) / prev_portfolio_value\n",
    "        else:\n",
    "            portfolio_return = 0\n",
    "\n",
    "        if portfolio_return > 0.01:\n",
    "            scaled_reward = portfolio_return * 150  \n",
    "        else:\n",
    "            scaled_reward = portfolio_return * 100\n",
    "\n",
    "        total_reward += scaled_reward  \n",
    "\n",
    "        if self.portfolio_value < 0:\n",
    "            self.portfolio_value = 0\n",
    "            self.balance = 0  \n",
    "            done = True  \n",
    "\n",
    "        if self.portfolio_value < self.initial_balance * (1 - self.max_loss):\n",
    "            done = True\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.portfolio_value_history.append(self.portfolio_value)\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, total_reward, done, {}\n",
    "\n",
    "# PPO를 위한 액터-크리틱 신경망 정의\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim_list, seq_length=20):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.timesnet = TimesNet(input_dim=7, seq_length=seq_length).to(device)  \n",
    "        self.policy_head = nn.ModuleList([nn.Linear(self.timesnet.embed_dim, action_dim) for action_dim in action_dim_list])\n",
    "        self.value_head = nn.Linear(self.timesnet.embed_dim * len(action_dim_list), 1)  \n",
    "        self.apply(self._weights_init)  \n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        stock_embeds = []\n",
    "        for i in range(len(tickers)):\n",
    "            start = i * (7 * self.seq_length + 1)\n",
    "            end = start + 7 * self.seq_length\n",
    "            seq = x[:, start:end]\n",
    "            seq = seq.view(-1, self.seq_length, 7)\n",
    "            embed = self.timesnet(seq)  \n",
    "            stock_embeds.append(embed)\n",
    "        policy_logits = [head(embed) for embed, head in zip(stock_embeds, self.policy_head)]  \n",
    "        combined_embeds = torch.cat(stock_embeds, dim=1)  \n",
    "        value = self.value_head(combined_embeds)  \n",
    "        return policy_logits, value\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.to(device)\n",
    "        policy_logits, _ = self.forward(state)\n",
    "        actions = []\n",
    "        action_logprobs = []\n",
    "        for logits in policy_logits:\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            actions.append(action.item())\n",
    "            action_logprob = dist.log_prob(action)\n",
    "            action_logprobs.append(action_logprob)\n",
    "        return np.array(actions), torch.stack(action_logprobs)\n",
    "\n",
    "    def evaluate(self, state, actions):\n",
    "        policy_logits, value = self.forward(state)\n",
    "        action_logprobs = []\n",
    "        dist_entropies = []\n",
    "        for i, logits in enumerate(policy_logits):\n",
    "            dist = Categorical(logits=logits)\n",
    "            action_logprob = dist.log_prob(actions[:, i])\n",
    "            dist_entropy = dist.entropy()\n",
    "            action_logprobs.append(action_logprob)\n",
    "            dist_entropies.append(dist_entropy)\n",
    "        return torch.stack(action_logprobs, dim=1), value.squeeze(-1), torch.stack(dist_entropies, dim=1)\n",
    "\n",
    "# 메모리 클래스 정의\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "# 진화 전략을 적용한 새로운 개체를 생성하는 클래스\n",
    "class EvolutionStrategy:\n",
    "    def __init__(self, population_size, mutation_rate=0.1, crossover_rate=0.5):\n",
    "        self.population_size = population_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.population = []\n",
    "\n",
    "    def initialize_population(self, policy_class, input_dim, action_dim_list, seq_length):\n",
    "        self.population = [policy_class(input_dim, action_dim_list, seq_length) for _ in range(self.population_size)]\n",
    "\n",
    "    def select_parents(self, fitness_scores):\n",
    "        total_fitness = sum(fitness_scores)\n",
    "        probabilities = [score / total_fitness for score in fitness_scores]\n",
    "        parents = np.random.choice(self.population, size=2, p=probabilities, replace=False)\n",
    "        return parents\n",
    "\n",
    "    def crossover(self, parent1, parent2):\n",
    "        child = ActorCritic(parent1.timesnet.embed_dim, [9] * env.stock_dim)\n",
    "        for child_param, param1, param2 in zip(child.parameters(), parent1.parameters(), parent2.parameters()):\n",
    "            prob = np.random.rand()\n",
    "            if prob < self.crossover_rate:\n",
    "                child_param.data.copy_(param1.data)\n",
    "            else:\n",
    "                child_param.data.copy_(param2.data)\n",
    "        return child\n",
    "\n",
    "    def mutate(self, policy):\n",
    "        for param in policy.parameters():\n",
    "            prob = np.random.rand()\n",
    "            if prob < self.mutation_rate:\n",
    "                param.data += torch.randn_like(param) * 0.02\n",
    "\n",
    "    def evolve(self, fitness_scores):\n",
    "        new_population = []\n",
    "        for _ in range(self.population_size // 2):\n",
    "            parent1, parent2 = self.select_parents(fitness_scores)\n",
    "            child1 = self.crossover(parent1, parent2)\n",
    "            child2 = self.crossover(parent2, parent1)\n",
    "            self.mutate(child1)\n",
    "            self.mutate(child2)\n",
    "            new_population.extend([child1, child2])\n",
    "        self.population = new_population\n",
    "\n",
    "# PPO 트레이너 클래스 정의\n",
    "class PPOTrainer:\n",
    "    def __init__(self, env, policy, memory, optimizer, gamma=0.99, epsilon=0.2, epochs=10, entropy_coef=0.05):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.memory = memory\n",
    "        self.optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epochs = epochs\n",
    "        self.entropy_coef = entropy_coef\n",
    "\n",
    "    def train(self, max_episodes):\n",
    "        for episode in range(max_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                actions, action_logprobs = self.policy.act(state_tensor)\n",
    "                next_state, reward, done, _ = self.env.step(actions)\n",
    "                total_reward += reward\n",
    "                self.memory.states.append(state)\n",
    "                self.memory.actions.append(actions)\n",
    "                self.memory.logprobs.append(action_logprobs.sum().item())\n",
    "                self.memory.rewards.append(reward)\n",
    "                self.memory.is_terminals.append(done)\n",
    "                state = next_state\n",
    "\n",
    "            self.ppo_update()\n",
    "            self.memory.clear()\n",
    "\n",
    "            self.plot_results(episode, total_reward)\n",
    "\n",
    "    def ppo_update(self):\n",
    "        if len(self.memory.states) == 0:\n",
    "            return\n",
    "\n",
    "        states = torch.tensor(self.memory.states, dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(self.memory.actions, dtype=torch.int64).to(device)\n",
    "        old_logprobs = torch.tensor(self.memory.logprobs, dtype=torch.float32).to(device)\n",
    "        rewards = self.memory.rewards\n",
    "        is_terminals = self.memory.is_terminals\n",
    "\n",
    "        returns = self.compute_returns(rewards, is_terminals)\n",
    "\n",
    "        advantages = self.compute_advantages(states, returns)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            action_logprobs, state_values, dist_entropies = self.policy.evaluate(states, actions)\n",
    "            total_logprobs = action_logprobs.sum(dim=1)\n",
    "            total_entropies = dist_entropies.sum(dim=1)\n",
    "            ratios = torch.exp(total_logprobs - old_logprobs)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "            value_loss = F.mse_loss(state_values.squeeze(-1), returns)\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * value_loss - self.entropy_coef * total_entropies\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss_mean = loss.mean()\n",
    "            loss_mean.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy.parameters(), max_norm=0.5)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {loss_mean.item():.4f}\")\n",
    "\n",
    "    def compute_returns(self, rewards, is_terminals):\n",
    "        returns = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(rewards), reversed(is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            returns.insert(0, discounted_reward)\n",
    "        return torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "    def compute_advantages(self, states, returns):\n",
    "        with torch.no_grad():\n",
    "            _, state_values = self.policy.forward(states)\n",
    "            advantages = returns - state_values.squeeze(-1)\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        return advantages\n",
    "\n",
    "    def plot_results(self, episode, total_reward):\n",
    "        action_counts = np.zeros(9)\n",
    "        for actions in self.env.action_history:\n",
    "            action_counts += np.bincount(actions, minlength=9)\n",
    "\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        plt.subplot(4, 1, 1)\n",
    "        plt.plot(self.env.portfolio_value_history)\n",
    "        plt.title(f'Episode {episode+1} - Portfolio Value Over Time')\n",
    "        plt.ylabel('Portfolio Value')\n",
    "\n",
    "        plt.subplot(4, 1, 2)\n",
    "        action_labels = ['Sell 100%', 'Sell 75%', 'Sell 50%', 'Sell 25%', 'Hold', \n",
    "                        'Buy 25%', 'Buy 50%', 'Buy 75%', 'Buy 100%']\n",
    "        plt.bar(action_labels, action_counts, color=['red', 'darkred', 'orange', 'lightcoral', 'gray', \n",
    "                                                    'lightgreen', 'green', 'darkgreen', 'lime'])\n",
    "        plt.title('Action Distribution')\n",
    "        plt.ylabel('Counts')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.subplot(4, 1, 3)\n",
    "        initial_prices = {ticker: self.env.price_history[ticker][0] for ticker in tickers}\n",
    "        final_prices = {ticker: self.env.price_history[ticker][-1] for ticker in tickers}\n",
    "        returns = []\n",
    "        for ticker in tickers:\n",
    "            if initial_prices[ticker] == 0:\n",
    "                ret = 0\n",
    "            else:\n",
    "                ret = (final_prices[ticker] - initial_prices[ticker]) / initial_prices[ticker] * 100\n",
    "            returns.append(ret)\n",
    "        plt.bar(tickers, returns, color=['blue', 'orange', 'purple'])\n",
    "        plt.title('Stock Returns (%)')\n",
    "        plt.ylabel('Return (%)')\n",
    "\n",
    "        plt.subplot(4, 1, 4)\n",
    "        hold_counts = [self.env.stock_owned[ticker]['quantity'] for ticker in tickers]\n",
    "        plt.bar(tickers, hold_counts, color=['cyan', 'magenta', 'yellow'])\n",
    "        plt.title('Current Holdings')\n",
    "        plt.ylabel('Number of Stocks')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Episode {episode+1} completed. Total Reward: {total_reward:.2f}, Final Portfolio Value: {self.env.portfolio_value_history[-1]:.2f}\")\n",
    "        print(f\"Action counts:\")\n",
    "        for i, label in enumerate(action_labels):\n",
    "            print(f\"  {label}: {int(action_counts[i])}\")\n",
    "        print('-' * 50)\n",
    "\n",
    "# 데이터 로드 및 환경 설정\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "dfs = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, start='2019-01-01', end='2023-12-31', progress=False)  \n",
    "    df.loc[:, 'Close_unscaled'] = df['Close']  \n",
    "\n",
    "    df.loc[:, 'MA10'] = df['Close'].rolling(window=10).mean()\n",
    "    df.loc[:, 'MA20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "    df.loc[:, 'Upper_Band'] = df['MA20'] + 2 * df['Close'].rolling(window=20).std()\n",
    "    df.loc[:, 'Lower_Band'] = df['MA20'] - 2 * df['Close'].rolling(window=20).std()\n",
    "\n",
    "    delta = df['Close'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    ema_up = up.ewm(com=13, adjust=False).mean()\n",
    "    ema_down = down.ewm(com=13, adjust=False).mean()\n",
    "    rs = ema_up / ema_down\n",
    "    df.loc[:, 'RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    df = df.dropna().copy()  \n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    feature_cols = ['Close', 'MA10', 'MA20', 'RSI', 'Volume', 'Upper_Band', 'Lower_Band']\n",
    "    df.loc[:, feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "    dfs[ticker] = df.reset_index(drop=True)\n",
    "\n",
    "env = MultiStockTradingEnv(dfs, initial_balance=5000000, seq_length=20)\n",
    "\n",
    "# 진화 전략 설정\n",
    "es = EvolutionStrategy(population_size=10)\n",
    "es.initialize_population(ActorCritic, env.observation_space.shape[0], [9] * env.stock_dim, seq_length=env.seq_length)\n",
    "\n",
    "# 진화 알고리즘과 PPO 통합 학습\n",
    "for generation in range(50):\n",
    "    fitness_scores = []\n",
    "    for policy in es.population:\n",
    "        optimizer = optim.Adam(policy.parameters(), lr=1e-5)  \n",
    "        memory = Memory()\n",
    "        trainer = PPOTrainer(env, policy, memory, optimizer)\n",
    "        trainer.train(max_episodes=5)\n",
    "        fitness_scores.append(env.portfolio_value_history[-1])\n",
    "\n",
    "    es.evolve(fitness_scores)\n",
    "\n",
    "    print(f\"Generation {generation + 1} completed. Best Portfolio Value: {max(fitness_scores):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

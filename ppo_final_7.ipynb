{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분데이터 적은 feature sequence 별로 scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 607\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    606\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# [1, state_dim]\u001b[39;00m\n\u001b[1;32m--> 607\u001b[0m     actions, action_logprobs \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;66;03m# actions = actions  # [stock_dim]\u001b[39;00m\n\u001b[0;32m    609\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "Cell \u001b[1;32mIn[18], line 445\u001b[0m, in \u001b[0;36mActorCritic.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m    444\u001b[0m     state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 445\u001b[0m     policy_logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(policy_logits, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m logits \u001b[38;5;129;01min\u001b[39;00m policy_logits:\n",
      "Cell \u001b[1;32mIn[18], line 415\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03mx: [batch_size, stock_dim * (4 * seq_length + 1) + 1]\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m stock_embeds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tickers)):\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# 각 종목의 시퀀스 데이터: [batch_size, 4 * seq_length]\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     start \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(using_cols) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_length)\n\u001b[0;32m    418\u001b[0m     end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(using_cols) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_length\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "# UserWarning 무시 (필요 시 제거 가능)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 여부 확인)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "seq_length = 20\n",
    "using_cols = ['Close', 'Power', 'RSI', '저가', 'MA5', 'MA20']\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_length, embed_dim)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(np.log(10000) / embed_dim))\n",
    "        \n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term) # 짝수\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term) # 홀수\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 입력 텐서 길이에 따라 포지셔널 인코딩 추가\n",
    "        # seq_len = x.size(1)\n",
    "        return x + self.encoding\n",
    "# transformer 모듈 정의 (트랜스포머 기반 시계열 처리 모델)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, seq_length, embed_dim=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_linear = nn.Linear(input_dim, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, max_length=seq_length)\n",
    "        \n",
    "        # TransformerEncoderLayer에 batch_first=True 설정\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # batch_first 설정\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_length, input_dim]\n",
    "        \"\"\"\n",
    "        # forward 메서드 내에서 입력 데이터 검증\n",
    "        if torch.isnan(x).any():\n",
    "            raise ValueError('입력 데이터에 NaN이 포함되어 있습니다.')\n",
    "        \n",
    "        x = self.input_linear(x)  # [batch_size, seq_length, embed_dim]\n",
    "        x = self.transformer_encoder(x)  # [batch_size, seq_length, embed_dim]\n",
    "        x = self.output_linear(x)  # [batch_size, seq_length, embed_dim]\n",
    "        x = self.activation(x)\n",
    "        # 시퀀스의 마지막 타임스탬프 출력\n",
    "        x = x[:, -1, :]           # [batch_size, embed_dim]\n",
    "        return x\n",
    "\n",
    "# 다중 종목 주식 트레이딩 환경 정의\n",
    "class MultiStockTradingEnv(gym.Env):\n",
    "    def __init__(self, dfs, stock_dim=3, initial_balance=1000000, max_stock=50, seq_length=20):\n",
    "        super(MultiStockTradingEnv, self).__init__()\n",
    "        self.dfs = dfs  # 종목별 데이터프레임 딕셔너리\n",
    "        self.stock_dim = stock_dim  # 종목 수\n",
    "        self.initial_balance = initial_balance  # 초기 자본금\n",
    "        self.max_stock = max_stock  # 각 종목당 최대 보유 주식 수\n",
    "        self.transaction_fee = 0.00015  # 거래 수수료 0.25%\n",
    "        self.national_tax = 0.0018\n",
    "        self.slippage = 0.003  # 슬리피지 0.3%\n",
    "        self.max_loss = 0.2  # 최대 허용 손실 (20%)\n",
    "        self.seq_length = seq_length  # 시퀀스 길이\n",
    "\n",
    "        # 상태 공간: 시퀀스 길이에 따른 각 종목의 [Close, MA10, MA20, RSI]와 보유 주식 수\n",
    "        # plus balance\n",
    "        # 각 종목: (4 * seq_length) + 1 (보유 주식 수)\n",
    "        # 총: stock_dim * (4 * seq_length +1) + 1 (balance)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.stock_dim * (len(using_cols) * self.seq_length + 1) + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.MultiDiscrete([9] * self.stock_dim)  # 각 종목별로 9개의 액션\n",
    "\n",
    "        # 초기화\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.current_step = 0\n",
    "        self.stock_owned = {ticker: {'quantity': 0, 'avg_price': 0} for ticker in self.dfs.keys()}\n",
    "        self.stock_price = {}\n",
    "        self.total_asset = []\n",
    "\n",
    "        # 히스토리 초기화\n",
    "        self.balance_history = [self.balance]\n",
    "        self.portfolio_value_history = [self.portfolio_value]\n",
    "        self.action_history = []\n",
    "        self.price_history = {ticker: [] for ticker in self.dfs.keys()}\n",
    "        self.trade_history = []  # 매수/매도 내역 저장\n",
    "\n",
    "        # 각 종목의 최대 스텝 수 계산\n",
    "        self.max_steps = min(len(df['original']) for _ , df in self.dfs.items()) - self.seq_length - 1\n",
    "        if self.max_steps <= 0:\n",
    "            raise ValueError(\"데이터프레임의 길이가 시퀀스 길이보다 짧습니다.\")\n",
    "\n",
    "        # 각 종목의 현재 인덱스 초기화\n",
    "        self.data_indices = {ticker: self.seq_length for ticker in self.dfs.keys()}  # 시퀀스 길이만큼 초기 인덱스\n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = []\n",
    "        for ticker in self.dfs.keys():\n",
    "            scaled_seq = self.dfs[ticker]['scaled_sequences'][self.current_step] # 현재 스텝에 해당하는 스케일된 시퀀스\n",
    "            # idx = self.data_indices[ticker]\n",
    "            # if idx < self.seq_length:\n",
    "            #     # 충분한 시퀀스가 없으면 제로 패딩\n",
    "            #     seq = df.loc[:idx, using_cols].values\n",
    "            #     pad_length = self.seq_length - seq.shape[0]\n",
    "            #     if pad_length > 0:\n",
    "            #         pad = np.zeros((pad_length, len(using_cols)))\n",
    "            #         seq = np.vstack((pad, seq))\n",
    "            # else:\n",
    "            #     # 시퀀스 슬라이싱\n",
    "            #     seq = df.loc[idx - self.seq_length:idx - 1, using_cols].values\n",
    "            obs.extend(scaled_seq.flatten())  # 시퀀스 데이터 추가\n",
    "            obs.append(self.stock_owned[ticker]['quantity'])  # 보유 주식 수량\n",
    "\n",
    "            # 현재 가격 저장 (iloc 사용)\n",
    "            self.stock_price[ticker] = self.dfs[ticker]['original'].iloc[self.current_step + seq_length - 1]['Close_unscaled']\n",
    "\n",
    "        # 잔고 추가\n",
    "        obs.append(self.balance)  # 잔고 추가\n",
    "\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "    def step(self, actions):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        invalid_action_penalty = -1000  # 불가능한 행동에 대한 페널티\n",
    "        self.action_history.append(actions)\n",
    "        trade_info = []  # 현재 스텝의 거래 내역\n",
    "\n",
    "        prev_portfolio_value = self.portfolio_value  # 이전 포트폴리오 가치 저장\n",
    "\n",
    "        # 액션 맵핑 정의\n",
    "        action_mapping = {\n",
    "            0: ('sell', 1.0),\n",
    "            1: ('sell', 0.75),\n",
    "            2: ('sell', 0.5),\n",
    "            3: ('sell', 0.25),\n",
    "            4: ('hold', 0.0),\n",
    "            5: ('buy', 0.25),\n",
    "            6: ('buy', 0.5),\n",
    "            7: ('buy', 0.75),\n",
    "            8: ('buy', 1.0)\n",
    "        }\n",
    "\n",
    "        # 종목별로 행동 수행\n",
    "        for i, (ticker, df) in enumerate(self.dfs.items()):\n",
    "            action = actions[i]\n",
    "            idx = self.data_indices[ticker]\n",
    "\n",
    "            # 인덱스가 데이터프레임의 범위를 벗어나면 에피소드를 종료\n",
    "            if idx >= len(df):\n",
    "                done = True\n",
    "                trade_info.append(f\"Ticker {ticker} reached end of data. Ending episode.\")\n",
    "                break\n",
    "\n",
    "            actual_price = df.iloc[idx]['Close_unscaled']\n",
    "\n",
    "            # 액션 맵핑을 통해 행동 타입과 비율 얻기\n",
    "            action_type, proportion = action_mapping.get(action, ('hold', 0.0))\n",
    "\n",
    "            # 슬리피지 적용\n",
    "            if action_type == 'buy':\n",
    "                adjusted_price = actual_price * (1 + self.slippage)\n",
    "            elif action_type == 'sell':\n",
    "                adjusted_price = actual_price * (1 - self.slippage)\n",
    "            else:\n",
    "                adjusted_price = actual_price\n",
    "\n",
    "            # 거래 수수료 계산\n",
    "            buy_fee = adjusted_price * self.transaction_fee\n",
    "            sell_fee = adjusted_price * (self.transaction_fee + self.national_tax)\n",
    "\n",
    "            # buy_amount과 sell_amount 초기화\n",
    "            buy_amount = 0\n",
    "            sell_amount = 0\n",
    "\n",
    "            # 행동에 따른 포트폴리오 업데이트 및 보상 계산\n",
    "            reward = 0  # 각 종목별 보상 초기화\n",
    "            if action_type == 'sell':\n",
    "                if self.stock_owned[ticker]['quantity'] > 0:\n",
    "                    sell_amount = int(self.stock_owned[ticker]['quantity'] * proportion)\n",
    "                    sell_amount = max(1, sell_amount)  # 최소 1주 매도\n",
    "                    sell_amount = min(sell_amount, self.stock_owned[ticker]['quantity'])  # 보유 주식 수 초과 방지\n",
    "                    proceeds = adjusted_price * sell_amount - sell_fee * sell_amount\n",
    "                    self.balance += proceeds\n",
    "                    # 이익 또는 손실 계산\n",
    "                    profit = (adjusted_price - self.stock_owned[ticker]['avg_price']) * sell_amount - sell_fee * sell_amount\n",
    "                    reward = profit  # 매도 시 보상은 이익 또는 손실\n",
    "                    self.stock_owned[ticker]['quantity'] -= sell_amount\n",
    "                    if self.stock_owned[ticker]['quantity'] == 0:\n",
    "                        self.stock_owned[ticker]['avg_price'] = 0\n",
    "                    trade_info.append(f\"Sell {sell_amount} of {ticker} at {adjusted_price:.2f}\")\n",
    "                else:\n",
    "                    # 보유한 주식이 없으면 페널티 부여\n",
    "                    reward = invalid_action_penalty\n",
    "                    trade_info.append(f\"Cannot Sell {ticker} (No holdings)\")\n",
    "            elif action_type == 'buy':\n",
    "                max_can_buy = min(\n",
    "                    self.max_stock - self.stock_owned[ticker]['quantity'],\n",
    "                    int(self.balance // (adjusted_price + buy_fee))\n",
    "                )\n",
    "                buy_amount = int(max_can_buy * proportion)\n",
    "                buy_amount = max(1, buy_amount)  # 최소 1주 매수\n",
    "                buy_amount = min(buy_amount, self.max_stock - self.stock_owned[ticker]['quantity'], \n",
    "                                 int(self.balance // (adjusted_price + buy_fee)))\n",
    "                if buy_amount > 0:\n",
    "                    cost = adjusted_price * buy_amount + buy_fee * buy_amount\n",
    "                    self.balance -= cost\n",
    "                    # 평균 매수가격 업데이트\n",
    "                    total_quantity = self.stock_owned[ticker]['quantity'] + buy_amount\n",
    "                    if total_quantity > 0:\n",
    "                        self.stock_owned[ticker]['avg_price'] = (\n",
    "                            (self.stock_owned[ticker]['avg_price'] * self.stock_owned[ticker]['quantity'] + adjusted_price * buy_amount)\n",
    "                            / total_quantity\n",
    "                        )\n",
    "                    self.stock_owned[ticker]['quantity'] = total_quantity\n",
    "                    reward = 0  # 매수 시 보상 없음\n",
    "                    trade_info.append(f\"Buy {buy_amount} of {ticker} at {adjusted_price:.2f}\")\n",
    "                else:\n",
    "                    reward = invalid_action_penalty  # 매수 불가 시 페널티\n",
    "                    trade_info.append(f\"Cannot Buy {ticker} (Insufficient balance or max stock)\")\n",
    "            else:\n",
    "                trade_info.append(f\"Hold {ticker}\")\n",
    "\n",
    "            # 보상 누적\n",
    "            total_reward += reward\n",
    "\n",
    "            # 가격 히스토리 업데이트\n",
    "            self.price_history[ticker].append(actual_price)\n",
    "\n",
    "            # 다음 인덱스로 이동\n",
    "            self.data_indices[ticker] += 1\n",
    "\n",
    "            # 디버깅을 위한 로그 출력\n",
    "            print(f\"Ticker: {ticker}, Action: {action_type}, Proportion: {proportion}, \"\n",
    "                  f\"Buy/Sell Amount: {buy_amount if action_type == 'buy' else sell_amount if action_type == 'sell' else 0}, \"\n",
    "                  f\"Balance: {self.balance:.2f}, Holdings: {self.stock_owned[ticker]['quantity']}, \"\n",
    "                  f'Total reward: {total_reward:.4f}')\n",
    "            \n",
    "            # 잔고와 보유 주식 수량이 음수가 되지 않도록 방지\n",
    "            if self.balance < 0:\n",
    "                print(f\"Warning: Balance for {ticker} is negative! Setting to 0.\")\n",
    "                self.balance = 0\n",
    "            if self.stock_owned[ticker]['quantity'] < 0:\n",
    "                print(f\"Warning: Holdings for {ticker} are negative! Setting to 0.\")\n",
    "                self.stock_owned[ticker]['quantity'] = 0\n",
    "                self.stock_owned[ticker]['avg_price'] = 0\n",
    "\n",
    "        # 현재 스텝의 거래 내역 저장 및 출력\n",
    "        if trade_info:\n",
    "            print(f\"Episode {self.current_step} Step {self.current_step}:\")\n",
    "            for info in trade_info:\n",
    "                print(info)\n",
    "            print()\n",
    "\n",
    "        self.trade_history.append(trade_info)\n",
    "\n",
    "        # 현재 포트폴리오 가치 계산\n",
    "        self.portfolio_value = self.balance + sum(\n",
    "            self.stock_owned[ticker]['quantity'] * self.stock_price[ticker] for ticker in self.dfs.keys()\n",
    "        )\n",
    "        self.total_asset.append(self.portfolio_value)\n",
    "\n",
    "        # 보상은 포트폴리오 가치의 비율적 변화량을 사용\n",
    "        if prev_portfolio_value > 0:\n",
    "            portfolio_return = (self.portfolio_value - prev_portfolio_value) / prev_portfolio_value  # 비율적 변화\n",
    "        else:\n",
    "            portfolio_return = 0  # 이전 포트폴리오 가치가 0일 경우\n",
    "        scaled_reward = portfolio_return * 100  # 예: 0.01 -> 1%\n",
    "        total_reward += scaled_reward  # 전체 보상에 추가\n",
    "\n",
    "        # 포트폴리오 가치가 음수가 되지 않도록 보장\n",
    "        if self.portfolio_value < 0:\n",
    "            print(f\"Error: Portfolio value is negative! Setting to 0.\")\n",
    "            self.portfolio_value = 0\n",
    "            self.balance = 0  # 잔고도 0으로 설정\n",
    "            done = True  # 에피소드를 종료\n",
    "\n",
    "        # 최대 허용 손실 초과 시 종료\n",
    "        if self.portfolio_value < self.initial_balance * (1 - self.max_loss):\n",
    "            done = True\n",
    "\n",
    "        # 최대 스텝 수 초과 시 종료\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        # 히스토리 업데이트\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.portfolio_value_history.append(self.portfolio_value)\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, total_reward, done, {}\n",
    "\n",
    "# 데이터 로드 및 기술적 지표 계산\n",
    "# tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "tickers = ['000660']\n",
    "dfs = {}\n",
    "for ticker in tickers:\n",
    "    # df = yf.download(ticker, start='2020-01-01', end='2020-12-31', progress=False)\n",
    "    df = pd.read_csv(f'./{ticker}.csv')\n",
    "    df = df.rename(columns={'종가': 'Close'})\n",
    "    df = df[60000:70000]\n",
    "    \n",
    "    # 원래의 Close 가격 보존\n",
    "    df.loc[:, 'Close_unscaled'] = df['Close']  # 실제 가격 저장\n",
    "\n",
    "    # 기술적 지표 계산\n",
    "    # df.loc[:, 'MA10'] = df['Close'].rolling(window=10).mean()\n",
    "    # df.loc[:, 'MA20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['Power'] = df['매수량'] - df['매도량']\n",
    "\n",
    "    # RSI 계산\n",
    "    delta = df['Close'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    ema_up = up.ewm(com=20, adjust=False).mean()\n",
    "    ema_down = down.ewm(com=20, adjust=False).mean()\n",
    "    rs = ema_up / ema_down\n",
    "    df.loc[:, 'RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    df = df.dropna().copy()  # 복사하여 경고 방지\n",
    "    \n",
    "    # Power 컬럼을 전체 데이터 기준으로 스케일링\n",
    "    scaler_power = StandardScaler()\n",
    "    df['Power'] = scaler_power.fit_transform(df[['Power']])\n",
    "    \n",
    "    # seq_length 단위로 나머지 컬럼 스케일링\n",
    "    scaled_sequences = []\n",
    "    for start in range(0, len(df) - seq_length + 1):\n",
    "        end = start + seq_length\n",
    "        \n",
    "        seq = df.iloc[start:end][using_cols] # Power는 이미 스케일링된 상태\n",
    "        features_to_scale = using_cols[3:]\n",
    "        # StandardScaler를 seq_length 단위로 적용 (Close 컬럼만)\n",
    "        scaler_close = StandardScaler()\n",
    "        scaled_close = scaler_close.fit_transform(seq[['Close']])\n",
    "        # 스케일링 후 NaN 체크\n",
    "        if np.isnan(scaled_close).any():\n",
    "            raise ValueError('스케일링된 Close 데이터에 NaN이 포함되어 있습니다.')\n",
    "        \n",
    "        seq[features_to_scale] = seq[features_to_scale].apply(lambda x: (x - scaler_close.mean_[0]) / scaler_close.scale_[0])\n",
    "        \n",
    "        # 스케일링된 Close와 원래의 Power 합치기\n",
    "        combined_seq = np.hstack((scaled_close, seq[using_cols[1:]].values))\n",
    "        scaled_sequences.append(combined_seq)\n",
    "\n",
    "    # 스케일된 데이터를 데이터프레임으로 변환\n",
    "    scaled_sequences = np.array(scaled_sequences)\n",
    "    \n",
    "    # 원본 데이터프레임에 스케일된 데이터 추가\n",
    "    dfs[ticker] = {\n",
    "        'original': df.reset_index(drop=True), # 원본 데이터\n",
    "        'scaled_sequences': scaled_sequences # 스케일된 시퀀스\n",
    "    }\n",
    "\n",
    "# 환경 생성 (초기 자본금 증가 및 시퀀스 길이 설정)\n",
    "env = MultiStockTradingEnv(dfs, stock_dim=len(tickers), initial_balance=100000000, seq_length=20)\n",
    "\n",
    "# PPO를 위한 액터-크리틱 신경망 정의\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim_list, seq_length=20):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        # self.input_dim = input_dim\n",
    "        self.transformer = Transformer(input_dim=len(using_cols), seq_length=seq_length).to(device)  # input_dim=4 (Close, MA10, MA20, RSI)\n",
    "        self.policy_head = nn.ModuleList([nn.Linear(self.transformer.embed_dim, action_dim) for action_dim in action_dim_list])\n",
    "        self.value_head = nn.Linear(self.transformer.embed_dim * len(action_dim_list), 1)  # embed_dim * stock_dim\n",
    "        self.apply(self._weights_init)  # 가중치 초기화\n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # 정책 헤드의 바이어스를 0으로 초기화하여 행동 확률 균등화\n",
    "            nn.init.zeros_(m.bias)\n",
    "            # nn.init.xavier_uniform_(m.weight)\\\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu') # ReLU초기화\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, stock_dim * (4 * seq_length + 1) + 1]\n",
    "        \"\"\"\n",
    "        stock_embeds = []\n",
    "        for i, in range(len(tickers)):\n",
    "            # 각 종목의 시퀀스 데이터: [batch_size, 4 * seq_length]\n",
    "            start = i * (len(using_cols) * self.seq_length)\n",
    "            end = start + len(using_cols) * self.seq_length\n",
    "            seq = x[:, start:end]\n",
    "            # Reshape to [batch_size, seq_length, 4]\n",
    "            seq = seq.view(-1, self.seq_length, len(using_cols))\n",
    "            embed = self.transformer(seq)  # [batch_size, embed_dim]\n",
    "            \n",
    "            # NaN 체크\n",
    "            if torch.isnan(embed).any():\n",
    "                raise ValueError('Transformer 출력에 NaN이 포함되어 있습니다.')\n",
    "            \n",
    "            stock_embeds.append(embed)\n",
    "            \n",
    "        # 정책 헤드는 각 embed를 처리하여 policy logits을 생성\n",
    "        policy_logits = [head(embed) for embed, head in zip(stock_embeds, self.policy_head)]  # [batch_size, 9] * stock_dim\n",
    "        \n",
    "        # NaN 체크\n",
    "        for logits in policy_logits:\n",
    "            if torch.isnan(logits).any():\n",
    "                raise ValueError('정책 네트워크의 출력에 NaN이 포함되어 있습니다.')\n",
    "        \n",
    "        # 가치 함수는 모든 embed를 합쳐 처리\n",
    "        combined_embeds = torch.cat(stock_embeds, dim=1)  # [batch_size, embed_dim * stock_dim]\n",
    "        value = self.value_head(combined_embeds)  # [batch_size, 1]\n",
    "        return policy_logits, value\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.to(device)\n",
    "        policy_logits, _ = self.forward(state)\n",
    "        if isinstance(policy_logits, list):\n",
    "            for logits in policy_logits:\n",
    "                if torch.isnan(logits).any():\n",
    "                    raise ValueError('정책 네트워크의 출력에 NaN이 포함되어 있습니다.')\n",
    "        else:\n",
    "            if torch.isnan(policy_logits).any():\n",
    "                raise ValueError(\"정책 네트워크의 출력에 NaN이 포함되어 있습니다.\")\n",
    "                \n",
    "        actions = []\n",
    "        action_logprobs = []\n",
    "        for logits in policy_logits:\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            actions.append(action.item())\n",
    "            action_logprob = dist.log_prob(action)\n",
    "            action_logprobs.append(action_logprob)\n",
    "        return np.array(actions), torch.stack(action_logprobs)\n",
    "\n",
    "    def evaluate(self, state, actions):\n",
    "        policy_logits, value = self.forward(state)\n",
    "        if isinstance(policy_logits, list):\n",
    "            for logits in policy_logits:\n",
    "                if torch.isnan(logits).any():\n",
    "                    raise ValueError('정책 네트워크의 출력에 NaN이 포함되어 있습니다.')\n",
    "        else:\n",
    "            if torch.isnan(policy_logits).any():\n",
    "                raise ValueError(\"정책 네트워크의 출력에 NaN이 포함되어 있습니다.\")\n",
    "        action_logprobs = []\n",
    "        dist_entropies = []\n",
    "        for i, logits in enumerate(policy_logits):\n",
    "            dist = Categorical(logits=logits)\n",
    "            action_logprob = dist.log_prob(actions[:, i])\n",
    "            dist_entropy = dist.entropy()\n",
    "            action_logprobs.append(action_logprob)\n",
    "            dist_entropies.append(dist_entropy)\n",
    "        return torch.stack(action_logprobs, dim=1), value.squeeze(-1), torch.stack(dist_entropies, dim=1)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 1e-5  # 학습률\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "epochs = 100  # 에폭 수\n",
    "\n",
    "# 정책 및 옵티마이저 초기화\n",
    "input_dim = env.observation_space.shape[0]\n",
    "action_dim_list = [9] * env.stock_dim  # 각 종목별 9개의 액션\n",
    "policy = ActorCritic(input_dim, action_dim_list, seq_length=env.seq_length).to(device)  # 모델을 GPU로 이동\n",
    "optimizer = optim.AdamW(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# 메모리 클래스 정의\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "# PPO 업데이트 함수 정의\n",
    "def ppo_update():\n",
    "    if len(memory.states) == 0:\n",
    "        return  # 메모리가 비어있으면 업데이트하지 않음\n",
    "\n",
    "    # 리스트를 텐서로 변환하고 GPU로 이동\n",
    "    states = torch.tensor(memory.states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(memory.actions, dtype=torch.int64).to(device)\n",
    "    old_logprobs = torch.tensor(memory.logprobs, dtype=torch.float32).to(device)\n",
    "    rewards = memory.rewards\n",
    "    is_terminals = memory.is_terminals\n",
    "\n",
    "    if torch.isnan(states).any():\n",
    "        raise ValueError('입력 상태에 NaN이 포함되어 있습니다.')\n",
    "    if torch.isnan(actions).any():\n",
    "        raise ValueError(\"액션에 NaN이 포함되어 있습니다.\")\n",
    "    if torch.isnan(old_logprobs).any():\n",
    "        raise ValueError(\"이전 로그 확률에 NaN이 포함되어 있습니다.\")\n",
    "        \n",
    "    # 리턴 계산 (할인 보상)\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for reward, is_terminal in zip(reversed(rewards), reversed(is_terminals)):\n",
    "        if is_terminal:\n",
    "            discounted_reward = 0\n",
    "        discounted_reward = reward + (gamma * discounted_reward)\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 어드밴티지 계산\n",
    "    with torch.no_grad():\n",
    "        _, state_values = policy.forward(states)\n",
    "        \n",
    "        # NaN 체크\n",
    "        if torch.isnan(state_values).any():\n",
    "            raise ValueError(\"상태 가치에 NaN이 포함되어 있습니다.\")\n",
    "        \n",
    "        advantages = returns - state_values.squeeze(-1)\n",
    "        # 어드밴티지 정규화\n",
    "        if advantages.std() == 0:\n",
    "            raise ValueError(\"어드밴티지의 표준편차가 0입니다.\")\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # 정책 업데이트\n",
    "    for epoch in range(epochs):\n",
    "        action_logprobs, state_values, dist_entropies = policy.evaluate(states, actions)\n",
    "        \n",
    "        if torch.isnan(action_logprobs).any() or torch.isnan(state_values).any():\n",
    "            raise ValueError('정책 업데이트 중 NaN이 포함되어 있습니다.')\n",
    "        \n",
    "        # 정책 헤드별 로그 확률 합계\n",
    "        total_logprobs = action_logprobs.sum(dim=1)\n",
    "        # 정책 헤드별 엔트로피 합계\n",
    "        total_entropies = dist_entropies.sum(dim=1)\n",
    "        ratios = torch.exp(total_logprobs - old_logprobs)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "        # 가치 함수 손실 추가\n",
    "        value_loss = F.mse_loss(state_values.squeeze(-1), returns)\n",
    "        \n",
    "        # 가치 손실에 NaN 체크\n",
    "        if torch.isnan(value_loss):\n",
    "            raise ValueError(\"가치 손실에 NaN이 포함되어 있습니다.\")\n",
    "        \n",
    "        entropy_coef = 0.01  # 엔트로피 계수 감소\n",
    "        loss = -torch.min(surr1, surr2) + 0.5 * value_loss - entropy_coef * total_entropies\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_mean = loss.mean()\n",
    "        # 손실 계산 후 NaN 체크\n",
    "        if torch.isnan(loss_mean):\n",
    "            raise ValueError('손실 값에 NaN이 포함되어 있습니다.')\n",
    "        loss_mean.backward()\n",
    "        \n",
    "\n",
    "        # 그레이디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 값 출력\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss_mean.item():.4f}\")\n",
    "\n",
    "# 초기화\n",
    "best_reward = float('-inf') # 최악의 초기 보상 설정\n",
    "\n",
    "# 학습 루프\n",
    "max_episodes = 1000  # 에피소드 수\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)  # [1, state_dim]\n",
    "        actions, action_logprobs = policy.act(state_tensor)\n",
    "        # actions = actions  # [stock_dim]\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "        total_reward += reward\n",
    "        # 메모리에 데이터 저장\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(actions)\n",
    "        memory.logprobs.append(action_logprobs.sum().item())\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "        state = next_state\n",
    "\n",
    "    # 정책 업데이트 및 메모리 초기화\n",
    "    ppo_update()\n",
    "    memory.clear()\n",
    "    \n",
    "    # 베스트 모델 저장\n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        torch.save(policy.state_dict(), 'best_ppo_actor_critic_model.pth')\n",
    "        print(f'Episode {episode + 1 }: New best model saved with total reward: {total_reward:.2f}')\n",
    "\n",
    "    # 행동 분포 시각화\n",
    "    action_counts = np.zeros(9)\n",
    "    for actions in env.action_history:\n",
    "        action_counts += np.bincount(actions, minlength=9)\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # 포트폴리오 가치 변화 시각화\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(env.portfolio_value_history)\n",
    "    plt.title(f'Episode {episode+1} - Portfolio Value Over Time')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "\n",
    "    # 행동 분포 시각화\n",
    "    plt.subplot(4, 1, 2)\n",
    "    action_labels = ['Sell 100%', 'Sell 75%', 'Sell 50%', 'Sell 25%', 'Hold', \n",
    "                    'Buy 25%', 'Buy 50%', 'Buy 75%', 'Buy 100%']\n",
    "    plt.bar(action_labels, action_counts, color=['red', 'darkred', 'orange', 'lightcoral', 'gray', \n",
    "                                                'lightgreen', 'green', 'darkgreen', 'lime'])\n",
    "    plt.title('Action Distribution')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # 종목별 수익률 시각화\n",
    "    plt.subplot(4, 1, 3)\n",
    "    initial_prices = {ticker: env.price_history[ticker][0] for ticker in tickers}\n",
    "    final_prices = {ticker: env.price_history[ticker][-1] for ticker in tickers}\n",
    "    returns = []\n",
    "    for ticker in tickers:\n",
    "        if initial_prices[ticker] == 0:\n",
    "            ret = 0\n",
    "        else:\n",
    "            ret = (final_prices[ticker] - initial_prices[ticker]) / initial_prices[ticker] * 100\n",
    "        returns.append(ret)\n",
    "    plt.bar(tickers, returns, color=['blue', 'orange', 'purple'])\n",
    "    plt.title('Stock Returns (%)')\n",
    "    plt.ylabel('Return (%)')\n",
    "\n",
    "    # 종목별 보유 주식 수 시각화\n",
    "    plt.subplot(4, 1, 4)\n",
    "    hold_counts = [env.stock_owned[ticker]['quantity'] for ticker in tickers]\n",
    "    plt.bar(tickers, hold_counts, color=['cyan', 'magenta', 'yellow'])\n",
    "    plt.title('Current Holdings')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Episode {episode+1} completed. Total Reward: {total_reward:.2f}, Final Portfolio Value: {env.portfolio_value_history[-1]:.2f}\")\n",
    "    print(f\"Action counts:\")\n",
    "    for i, label in enumerate(action_labels):\n",
    "        print(f\"  {label}: {int(action_counts[i])}\")\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
